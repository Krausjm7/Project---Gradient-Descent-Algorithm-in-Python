{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "#Machine Learning (Stanford University Andrew Ng - Coursera)\n#Project - Gradient Descent Algorthim - Solution by Jamie Kraus\n\nimport math, copy\nimport numpy as ny\nimport matplotlib.pyplot as mlp\nmlp.style.use('./deeplearning.mplstyle')\nfrom lab_utils_uni import mlp_house_x, mlp_contour_wgrad, mlp_divergence, mlp_gradients\n\n#Training Data Set - A house with 1000 square feet (x column) sold for $275,000 (y column) and a house with 2000 square feet (x column) sold for $450,000 (y column).\nx_train = ny.array([1000, 2000])       #Houses\ny_train = ny.array([275000, 450000])   #Sold Value\n\n#Objective Function Calculating Cost\ndef compute_cost(x, y, z, a):\n   \n    c = x.shape[0] \n    cost = 0\n    \n    for i in range(c):\n        f_za = z * x[i] + a\n        cost = cost + (f_za - y[i])**2\n    total_cost = 1 / (2 * c) * cost\n\n    return total_cost\n\n#Compute Gradient Descent Function to Pass to Gradient_Function\ndef compute_gradient(x, y, z, a): \n    \n    # Training Set\n    c = x.shape[0]    \n    dc_dz = 0\n    dc_da = 0\n    \n    for i in range(c):  \n        f_za = z * x[i] + a \n        dc_dz_i = (f_za - y[i]) * x[i] \n        dc_da_i = f_za - y[i] \n        dc_da += dc_da_i\n        dc_dz += dc_dz_i \n    dc_dz = dc_dz / c \n    dc_da = dc_da / c \n        \n    return dc_dz, dc_da\n\n#Gradient Descent Algorithm\ndef gradient_descent(x, y, z_in, a_in, alpha, num_iters, cost_function, gradient_function): \n    a = a_in\n    z = z_in\n\n    # Defining Arrays\n    c_history = []\n    p_history = []\n    \n    for i in range(num_iters):\n        # Calculating the gradient and Updating Parameters using gradient_function\n        dc_dz, dc_da = gradient_function(x, y, z, a)     \n\n        # Update Parameters\n        a = a - alpha * dc_da                            \n        z = z - alpha * dc_dz\n\n        # Save cost c at each iteration\n        if i<100000:   \n            c_history.append(cost_function(x, y, z, a))\n            p_history.append([z,a])\n       \n        # Print cost at intervals of 10\n        if i% math.ceil(num_iters/10) == 0:\n            print(f\"Iteration {i:4}: Cost {c_history[-1]:0.25e} \",\n                  f\"dc_dz: {dc_dz: 0.35e}, dc_da: {dc_da: 0.35e}  \",\n                  f\"z: {z: 0.35e}, a:{a: 0.45e}\")\n \n    return z, a, c_history, p_history \n\n# initialize parameters\nz_init = 0\na_init = 0\n# Gradient descent settings\niterations = 10000\ntmp_alpha = 1.0e-2\n# run gradient descent\nz_final, a_final, c_history, p_history = gradient_descent(x_train ,y_train, z_init, a_init, tmp_alpha, \n                                                    iterations, compute_cost, compute_gradient)\nprint(f\"(z,a) found by gradient descent: ({z_final:8.4f},{a_final:8.4f})\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}